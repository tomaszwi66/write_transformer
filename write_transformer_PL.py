#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘   TRANSFORMER OD ZERA â€” KURS KROK PO KROKU                      â•‘
â•‘                                                                  â•‘
â•‘   Po przejÅ›ciu tego kursu bÄ™dziesz umiaÅ‚ sam od zera             â•‘
â•‘   napisaÄ‡ i uruchomiÄ‡ Transformer / GPT.                         â•‘
â•‘                                                                  â•‘
â•‘   11 krokÃ³w. KaÅ¼dy krok:                                         â•‘
â•‘   1. WyjaÅ›nienie â€” co to jest, dlaczego, analogia                â•‘
â•‘   2. Kod â€” gotowy, skomentowany, do przepisania                  â•‘
â•‘   3. Demo â€” uruchamia ten kawaÅ‚ek i pokazuje wynik               â•‘
â•‘   4. Quiz â€” sprawdza zrozumienie                                 â•‘
â•‘   5. Enter â†’ nastÄ™pny krok                                       â•‘
â•‘                                                                  â•‘
â•‘   Na koÅ„cu: wszystko dziaÅ‚a razem â€” trenujesz model              â•‘
â•‘   na SWOIM tekÅ›cie i generujesz.                                 â•‘
â•‘                                                                  â•‘
â•‘   Wymagania: pip install torch numpy                             â•‘
â•‘                                                                  â•‘
â•‘   Uruchomienie:                                                  â•‘
â•‘     python write_transformer.py              # kurs krok po krokuâ•‘
â•‘     python write_transformer.py --train plik.txt   # od razu trenâ•‘
â•‘     python write_transformer.py --interactive      # generowanie â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import time
import os
import sys
import argparse
import json
import re
from collections import Counter


# ================================================================
#  KONFIGURACJA
# ================================================================
DEFAULT_CONFIG = {
    "vocab_size": 0,
    "d_model": 64,
    "n_heads": 4,
    "n_layers": 4,
    "d_ff": 256,
    "max_seq_len": 128,
    "dropout": 0.1,
    "batch_size": 16,
    "epochs": 30,
    "lr": 3e-4,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "grad_clip": 1.0,
    "eval_interval": 5,
    "bpe_vocab_size": 512,
    "temperature": 0.8,
    "top_k": 40,
    "top_p": 0.9,
    "max_gen_len": 200,
}

DEFAULT_TEXT = """
Kot siedzi na macie i obserwuje ptaki za oknem. Ptaki Å›piewajÄ… piÄ™kne melodie.
Pies leÅ¼y na dywanie obok kominka. OgieÅ„ trzaska cicho w kominku.
MaÅ‚y kot goni duÅ¼ego psa po ogrodzie. BawiÄ… siÄ™ razem do zmroku.
Stary czÅ‚owiek czyta ksiÄ…Å¼kÄ™ w cichej bibliotece. Biblioteka jest spokojna.
MÅ‚oda dziewczyna pisze opowiadanie o dzielnym rycerzu. Rycerz ratuje krÃ³lestwo.
Deszcz pada Å‚agodnie na zielonÄ… Å‚Ä…kÄ™. Kwiaty kwitnÄ… wiosnÄ… we wszystkich kolorach.
SÅ‚oÅ„ce Å›wieci jasno na bezchmurnym niebie. Chmury dryfujÄ… powoli nad miastem.
Nauczyciel wyjaÅ›nia lekcjÄ™ uczniom w klasie. Uczniowie sÅ‚uchajÄ… uwaÅ¼nie.
Kucharz gotuje pyszne danie w duÅ¼ej kuchni. Kuchnia pachnie cudownie.
Muzyk gra piÄ™knÄ… melodiÄ™ na starym fortepianie. PublicznoÅ›Ä‡ sÅ‚ucha w ciszy.
Naukowiec odkrywa nowÄ… formuÅ‚Ä™ w laboratorium. Odkrycie zmienia wszystko.
Malarz tworzy arcydzieÅ‚o jasnymi kolorami. Galeria wystawia obraz na miejscu.
Rolnik uprawia Å›wieÅ¼e warzywa na rozlegÅ‚ym polu. Zbiory sÄ… obfite tego roku.
Dzieci bawiÄ… siÄ™ radoÅ›nie w parku po szkole. ÅšmiejÄ… siÄ™ i biegajÄ… razem.
Lekarz bada pacjenta dokÅ‚adnie. Pacjent czuje siÄ™ duÅ¼o lepiej po wizycie.
Rybak Å‚owi wiele ryb w gÅ‚Ä™bokim morzu. ÅÃ³dka koÅ‚ysze siÄ™ Å‚agodnie na falach.
Pisarz pracuje nad nowÄ… powieÅ›ciÄ… kaÅ¼dego ranka. Historia roÅ›nie strona po stronie.
Ogrodnik sadzi piÄ™kne rÃ³Å¼e w ogrodzie. RÃ³Å¼e kwitnÄ… w kolorze czerwonym i biaÅ‚ym.
Astronom obserwuje gwiazdy przez teleskop. Nocne niebo jest wspaniaÅ‚e i tajemnicze.
Piekarz robi Å›wieÅ¼y chleb kaÅ¼dego ranka przed Å›witem. Piekarnia pachnie cudownie.
PodrÃ³Å¼nik odkrywa nowe kraje i kultury. KaÅ¼da podrÃ³Å¼ uczy czegoÅ› nowego.
Student uczy siÄ™ pilnie do waÅ¼nego egzaminu. CiÄ™Å¼ka praca prowadzi do sukcesu.
Architekt projektuje nowoczesny budynek dla miasta. Projekt jest innowacyjny.
Pilot leci samolotem nad rozlegÅ‚ym oceanem. Widok z gÃ³ry jest zapierajÄ…cy dech.
Bibliotekarka porzÄ…dkuje tysiÄ…ce ksiÄ…Å¼ek na pÃ³Å‚kach. Wiedza wypeÅ‚nia kaÅ¼dy kÄ…t.
"""


# ================================================================
#  NARZÄ˜DZIA KURSU
# ================================================================

def clear_screen():
    os.system('cls' if os.name == 'nt' else 'clear')


def wait(msg="NaciÅ›nij Enter aby kontynuowaÄ‡..."):
    try:
        input(f"\n  â {msg}")
    except (EOFError, KeyboardInterrupt):
        print("\n  Przerwano kurs.")
        sys.exit(0)


def show_header(step, total, title):
    print(f"\n{'â•'*65}")
    print(f"  KROK {step}/{total}: {title}")
    print(f"{'â•'*65}")


def show_explanation(text):
    print()
    for line in text.strip().split('\n'):
        print(f"  {line}")


def show_code(code):
    print(f"\n  {'â”€'*60}")
    print(f"  ğŸ“ KOD DO PRZEPISANIA:")
    print(f"  {'â”€'*60}")
    for line in code.strip().split('\n'):
        print(f"  â”‚ {line}")
    print(f"  {'â”€'*60}")


def show_demo(title):
    print(f"\n  ğŸ”¬ DEMO: {title}")
    print(f"  {'Â·'*50}")


def quiz(question, options, correct, explanation):
    print(f"\n  â“ {question}")
    for i, opt in enumerate(options):
        print(f"     {i+1}. {opt}")
    try:
        ans = input("     Twoja odpowiedÅº (1-4): ").strip()
        if ans == str(correct):
            print(f"     âœ… Poprawnie!")
        else:
            print(f"     âŒ OdpowiedÅº: {correct}. {options[correct-1]}")
        print(f"     ğŸ’¡ {explanation}")
    except (EOFError, KeyboardInterrupt):
        print(f"\n     OdpowiedÅº: {correct}. {options[correct-1]}")


# ================================================================
#  KROK 1: TOKENIZER BPE
# ================================================================

class BPETokenizer:
    """Byte-Pair Encoding â€” ten sam algorytm co w GPT-2."""

    def __init__(self, vocab_size=512):
        self.target_vocab_size = vocab_size
        self.merges = {}
        self.vocab = {}
        self.inverse_vocab = {}
        self.trained = False
        self.pad_token = "<PAD>"
        self.bos_token = "<BOS>"
        self.eos_token = "<EOS>"
        self.unk_token = "<UNK>"

    def train(self, text, verbose=True):
        if verbose:
            print(f"     TrenujÄ™ BPE... (tekst: {len(text):,} znakÃ³w)")

        self.vocab = {
            self.pad_token: 0, self.bos_token: 1,
            self.eos_token: 2, self.unk_token: 3,
        }

        chars = sorted(set(text))
        for ch in chars:
            if ch not in self.vocab:
                self.vocab[ch] = len(self.vocab)

        words = text.split()
        word_freqs = Counter(words)

        splits = {}
        for word, freq in word_freqs.items():
            splits[tuple(word) + ('</w>',)] = freq

        if '</w>' not in self.vocab:
            self.vocab['</w>'] = len(self.vocab)

        num_merges = self.target_vocab_size - len(self.vocab)
        self.merges = {}

        for merge_idx in range(num_merges):
            pair_counts = Counter()
            for word_tokens, freq in splits.items():
                for i in range(len(word_tokens) - 1):
                    pair_counts[(word_tokens[i], word_tokens[i+1])] += freq

            if not pair_counts:
                break

            best_pair = pair_counts.most_common(1)[0][0]
            if pair_counts[best_pair] < 2:
                break

            merged = best_pair[0] + best_pair[1]
            self.merges[best_pair] = merged
            if merged not in self.vocab:
                self.vocab[merged] = len(self.vocab)

            new_splits = {}
            for word_tokens, freq in splits.items():
                new_word = []
                i = 0
                while i < len(word_tokens):
                    if (i < len(word_tokens) - 1 and
                            word_tokens[i] == best_pair[0] and
                            word_tokens[i+1] == best_pair[1]):
                        new_word.append(merged)
                        i += 2
                    else:
                        new_word.append(word_tokens[i])
                        i += 1
                new_splits[tuple(new_word)] = freq
            splits = new_splits

            if verbose and (merge_idx + 1) % 100 == 0:
                print(f"     Scalenie {merge_idx+1}: "
                      f"'{best_pair[0]}'+'{best_pair[1]}'â†’'{merged}'")

        self.inverse_vocab = {v: k for k, v in self.vocab.items()}
        self.trained = True

        if verbose:
            print(f"     âœ… SÅ‚ownik: {len(self.vocab)} tokenÃ³w, "
                  f"{len(self.merges)} scaleÅ„")

    def _apply_merges(self, tokens):
        while True:
            best_pair = None
            best_merge_rank = len(self.merges)
            merge_keys = list(self.merges.keys())

            for i in range(len(tokens) - 1):
                pair = (tokens[i], tokens[i+1])
                if pair in self.merges:
                    rank = merge_keys.index(pair)
                    if rank < best_merge_rank:
                        best_merge_rank = rank
                        best_pair = pair

            if best_pair is None:
                break

            merged = self.merges[best_pair]
            new_tokens = []
            i = 0
            while i < len(tokens):
                if (i < len(tokens) - 1 and
                        tokens[i] == best_pair[0] and
                        tokens[i+1] == best_pair[1]):
                    new_tokens.append(merged)
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text):
        if not self.trained:
            raise RuntimeError("Tokenizer nie wytrenowany!")
        ids = [self.vocab[self.bos_token]]
        for word in text.split():
            tokens = list(word) + ['</w>']
            tokens = self._apply_merges(tokens)
            for t in tokens:
                ids.append(self.vocab.get(t, self.vocab[self.unk_token]))
        ids.append(self.vocab[self.eos_token])
        return ids

    def decode(self, ids):
        tokens = []
        for id in ids:
            t = self.inverse_vocab.get(id, self.unk_token)
            if t in (self.pad_token, self.bos_token, self.eos_token):
                continue
            tokens.append('?' if t == self.unk_token else t)
        return ''.join(tokens).replace('</w>', ' ').strip()

    def save(self, path):
        data = {
            "vocab": self.vocab,
            "merges": {f"{k[0]}|||{k[1]}": v for k, v in self.merges.items()},
            "target_vocab_size": self.target_vocab_size,
        }
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.vocab = {k: int(v) for k, v in data["vocab"].items()}
        self.inverse_vocab = {v: k for k, v in self.vocab.items()}
        self.merges = {}
        for k, v in data["merges"].items():
            parts = k.split("|||")
            self.merges[(parts[0], parts[1])] = v
        self.target_vocab_size = data["target_vocab_size"]
        self.trained = True


def step_1_tokenizer(text):
    show_header(1, 11, "TOKENIZER BPE")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CO TO JEST?                                                 â•‘
â•‘                                                              â•‘
â•‘  Komputer nie rozumie tekstu. Rozumie liczby.                â•‘
â•‘  Tokenizer zamienia tekst na liczby i z powrotem.            â•‘
â•‘                                                              â•‘
â•‘  "kot siedzi na macie"                                       â•‘
â•‘       â†“ encode()                                             â•‘
â•‘  [1, 45, 23, 67, 89, 2]                                     â•‘
â•‘       â†“ model przetwarza                                     â•‘
â•‘  [1, 45, 23, 67, 89, 34, 2]                                 â•‘
â•‘       â†“ decode()                                             â•‘
â•‘  "kot siedzi na macie dywanie"                               â•‘
â•‘                                                              â•‘
â•‘  RODZAJE TOKENIZERÃ“W:                                        â•‘
â•‘  â€¢ Word-level: 1 sÅ‚owo = 1 token (prosty, duÅ¼y sÅ‚ownik)     â•‘
â•‘  â€¢ BPE: subword â€” "nieszczÄ™Å›liwy" â†’ "nie"+"szczÄ™Å›liwy"      â•‘
â•‘    â†‘ TEGO UÅ»YWA GPT-2, GPT-3, GPT-4                         â•‘
â•‘  â€¢ SentencePiece: statystyczny (LLaMA, T5)                   â•‘
â•‘                                                              â•‘
â•‘  ALGORYTM BPE:                                               â•‘
â•‘  1. Zacznij od pojedynczych znakÃ³w: ['k','o','t']            â•‘
â•‘  2. Policz pary: ('k','o') wystÄ™puje 15 razy                â•‘
â•‘  3. Scal najczÄ™stszÄ…: 'k'+'o' â†’ 'ko'                        â•‘
â•‘  4. Powtarzaj aÅ¼ masz docelowy rozmiar sÅ‚ownika             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
class BPETokenizer:
    def __init__(self, vocab_size=512):
        self.vocab = {"<PAD>": 0, "<BOS>": 1, "<EOS>": 2, "<UNK>": 3}
        self.merges = {}  # (token_a, token_b) â†’ scalony_token

    def train(self, text):
        # 1. Dodaj wszystkie unikalne znaki do sÅ‚ownika
        for ch in sorted(set(text)):
            self.vocab[ch] = len(self.vocab)

        # 2. Podziel kaÅ¼de sÅ‚owo na znaki + marker koÅ„ca
        # "kot" â†’ ('k', 'o', 't', '</w>')

        # 3. W pÄ™tli:
        #    a) Policz wszystkie sÄ…siednie pary
        #    b) Scal najczÄ™stszÄ… parÄ™
        #    c) Dodaj do sÅ‚ownika
        #    d) PowtÃ³rz aÅ¼ vocab_size

    def encode(self, text):
        # SÅ‚owo â†’ znaki â†’ zastosuj scalenia â†’ zamieÅ„ na ID
        return [self.vocab[token] for token in tokens]

    def decode(self, ids):
        # ID â†’ tokeny â†’ sklej â†’ zamieÅ„ '</w>' na spacje
        return text
    """)

    wait("Enter â†’ zobacz tokenizer w akcji...")

    show_demo("Tokenizer BPE")

    tokenizer = BPETokenizer(vocab_size=256)
    tokenizer.train(text)

    test = "Kot siedzi na macie"
    ids = tokenizer.encode(test)
    decoded = tokenizer.decode(ids)

    print(f"\n     Tekst:      '{test}'")
    print(f"     Zakodowane: {ids}")
    print(f"     Dekodowane: '{decoded}'")
    print(f"     Rozmiar sÅ‚ownika: {len(tokenizer.vocab)}")
    print(f"     Kompresja: {len(test)}/{len(ids)} = "
          f"{len(test)/len(ids):.1f} znakÃ³w/token")

    quiz(
        "Co robi BPE inaczej niÅ¼ tokenizer word-level?",
        [
            "Dzieli tekst na zdania",
            "Dzieli rzadkie sÅ‚owa na mniejsze znane kawaÅ‚ki",
            "Zamienia tekst na binarne",
            "Usuwa znaki specjalne"
        ],
        2,
        "BPE dzieli rzadkie sÅ‚owa na podwyrazy. 'nieszczÄ™Å›liwy' â†’ "
        "'nie'+'szczÄ™Å›liwy'. DziÄ™ki temu radzi sobie z nowymi sÅ‚owami."
    )

    return tokenizer


# ================================================================
#  KROK 2: PIPELINE DANYCH
# ================================================================

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, token_ids, seq_len):
        self.seq_len = seq_len
        self.data = torch.tensor(token_ids, dtype=torch.long)
        self.n_examples = max(0, len(self.data) - seq_len)

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        chunk = self.data[idx: idx + self.seq_len + 1]
        return chunk[:-1], chunk[1:]


def step_2_data_pipeline(tokenizer, text):
    show_header(2, 11, "PIPELINE DANYCH")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CO TO JEST?                                                 â•‘
â•‘                                                              â•‘
â•‘  Model uczy siÄ™ PRZEWIDYWAÄ† NASTÄ˜PNY TOKEN.                  â•‘
â•‘  To jedyny cel GPT. CaÅ‚a "inteligencja" z tego wynika.       â•‘
â•‘                                                              â•‘
â•‘  JAK TWORZYMY DANE TRENINGOWE?                               â•‘
â•‘                                                              â•‘
â•‘  Mamy ztokenizowany tekst: [10, 20, 30, 40, 50, 60, 70]     â•‘
â•‘                                                              â•‘
â•‘  Tniemy go na nakÅ‚adajÄ…ce siÄ™ okna (seq_len=4):              â•‘
â•‘                                                              â•‘
â•‘  PrzykÅ‚ad 1:  WejÅ›cie: [10, 20, 30, 40]                     â•‘
â•‘               Cel:     [20, 30, 40, 50]                      â•‘
â•‘                                                              â•‘
â•‘  PrzykÅ‚ad 2:  WejÅ›cie: [20, 30, 40, 50]                     â•‘
â•‘               Cel:     [30, 40, 50, 60]                      â•‘
â•‘                                                              â•‘
â•‘  Na KAÅ»DEJ pozycji model przewiduje nastÄ™pny token:          â•‘
â•‘                                                              â•‘
â•‘  WejÅ›cie: [Kot,  siedzi, na,    ?   ]                        â•‘
â•‘  Cel:     [siedzi, na,   macie, EOS ]                        â•‘
â•‘            â†‘       â†‘     â†‘      â†‘                            â•‘
â•‘  Model musi zgadnÄ…Ä‡ kaÅ¼dy z tych tokenÃ³w!                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
class TextDataset(torch.utils.data.Dataset):
    def __init__(self, token_ids, seq_len):
        self.data = torch.tensor(token_ids, dtype=torch.long)
        self.n_examples = len(self.data) - seq_len
        # Ile okien mieÅ›ci siÄ™ w tekÅ›cie

    def __getitem__(self, idx):
        chunk = self.data[idx : idx + seq_len + 1]
        x = chunk[:-1]    # wejÅ›cie: wszystko oprÃ³cz ostatniego
        y = chunk[1:]      # cel: wszystko oprÃ³cz pierwszego
        return x, y

# DataLoader automatycznie grupuje przykÅ‚ady w batche:
loader = DataLoader(dataset, batch_size=16, shuffle=True)
    """)

    wait("Enter â†’ zobacz pipeline w akcji...")

    show_demo("Pipeline danych")

    token_ids = tokenizer.encode(re.sub(r'\s+', ' ', text).strip())
    dataset = TextDataset(token_ids, seq_len=8)

    x, y = dataset[0]
    x_words = [tokenizer.inverse_vocab.get(i.item(), '?') for i in x]
    y_words = [tokenizer.inverse_vocab.get(i.item(), '?') for i in y]

    print(f"\n     TokenÃ³w w korpusie: {len(token_ids):,}")
    print(f"     PrzykÅ‚adÃ³w (seq_len=8): {len(dataset):,}")
    print(f"\n     PrzykÅ‚ad 1:")
    print(f"       WejÅ›cie (x): {x.tolist()}")
    print(f"       Tokeny:      {x_words}")
    print(f"       Cel (y):     {y.tolist()}")
    print(f"       Tokeny:      {y_words}")
    print(f"\n     â†‘ Model widzi x i musi przewidzieÄ‡ y")
    print(f"       Na pozycji 0: widzi '{x_words[0]}' â†’ cel: '{y_words[0]}'")
    print(f"       Na pozycji 1: widzi '{x_words[0]},{x_words[1]}' â†’ cel: '{y_words[1]}'")

    quiz(
        "Dlaczego cel jest przesuniÄ™ty o 1 w prawo?",
        [
            "Bo model ma przewidywaÄ‡ poprzedni token",
            "Bo model ma przewidywaÄ‡ NASTÄ˜PNY token â€” cel to przyszÅ‚oÅ›Ä‡",
            "Å»eby zaoszczÄ™dziÄ‡ pamiÄ™Ä‡",
            "To jest bÅ‚Ä…d w kodzie"
        ],
        2,
        "Cel to sekwencja przesuniÄ™ta o 1 w prawo. Na kaÅ¼dej pozycji "
        "model musi przewidzieÄ‡ co bÄ™dzie DALEJ. To jedyny cel GPT!"
    )

    return token_ids


# ================================================================
#  KROK 3: EMBEDDINGI
# ================================================================

class TransformerEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model, max_seq_len, dropout=0.1):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.pos_emb = nn.Embedding(max_seq_len, d_model)
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model

    def forward(self, x):
        seq_len = x.size(1)
        tok = self.token_emb(x)
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)
        pos = self.pos_emb(positions)
        return self.dropout(tok + pos)


def step_3_embeddings(tokenizer):
    show_header(3, 11, "EMBEDDINGI (TOKEN + POZYCJA)")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CO TO JEST?                                                 â•‘
â•‘                                                              â•‘
â•‘  Token ID to tylko liczba (np. 42). Model potrzebuje         â•‘
â•‘  WEKTORA â€” listy liczb, z ktÃ³rymi moÅ¼e liczyÄ‡.               â•‘
â•‘                                                              â•‘
â•‘  EMBEDDING TOKENÃ“W:                                          â•‘
â•‘  ID 42 ("kot") â†’ [0.12, -0.34, 0.56, 0.78, ...]            â•‘
â•‘  ID 15 ("pies") â†’ [0.11, -0.31, 0.58, 0.75, ...]           â•‘
â•‘  â†‘ Podobne sÅ‚owa â†’ podobne wektory (model tego siÄ™ uczy!)   â•‘
â•‘                                                              â•‘
â•‘  EMBEDDING POZYCYJNY:                                        â•‘
â•‘  Attention nie wie o kolejnoÅ›ci â€” traktuje tokeny jak ZBIÃ“R.  â•‘
â•‘  Musimy powiedzieÄ‡ modelowi GDZIE jest kaÅ¼dy token.          â•‘
â•‘                                                              â•‘
â•‘  Pozycja 0 â†’ [0.00, 1.00, 0.00, 1.00, ...]                 â•‘
â•‘  Pozycja 1 â†’ [0.84, 0.54, 0.01, 0.99, ...]                 â•‘
â•‘  Pozycja 2 â†’ [0.91, 0.42, 0.02, 0.98, ...]                 â•‘
â•‘                                                              â•‘
â•‘  WYNIK = embedding_tokena + embedding_pozycji                â•‘
â•‘                                                              â•‘
â•‘  GPT-2 uÅ¼ywa UCZONYCH embeddingÃ³w pozycyjnych (my teÅ¼).      â•‘
â•‘  Oryginalny Transformer uÅ¼ywaÅ‚ sinusoidalnych (staÅ‚ych).     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
class TransformerEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model, max_seq_len, dropout=0.1):
        super().__init__()
        # Tablica wyszukiwania: ID tokena â†’ wektor d_model wymiarÃ³w
        self.token_emb = nn.Embedding(vocab_size, d_model)

        # Tablica wyszukiwania: pozycja â†’ wektor d_model wymiarÃ³w
        self.pos_emb = nn.Embedding(max_seq_len, d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: [batch, seq_len] â€” ID tokenÃ³w
        tok = self.token_emb(x)                      # [batch, seq_len, d_model]
        positions = torch.arange(seq_len).unsqueeze(0) # [1, seq_len]
        pos = self.pos_emb(positions)                  # [1, seq_len, d_model]
        return self.dropout(tok + pos)                 # suma!
    """)

    wait("Enter â†’ zobacz embeddingi w akcji...")

    show_demo("Embeddingi")

    vocab_size = len(tokenizer.vocab)
    d_model = 32
    emb = TransformerEmbedding(vocab_size, d_model, max_seq_len=64)

    test_ids = tokenizer.encode("Kot siedzi na macie")
    x = torch.tensor([test_ids])
    output = emb(x)

    print(f"\n     WejÅ›cie (ID tokenÃ³w): {test_ids}")
    print(f"     KsztaÅ‚t wejÅ›cia:  {x.shape}  (batch=1, seq_len={len(test_ids)})")
    print(f"     KsztaÅ‚t wyjÅ›cia: {output.shape}  (batch=1, seq_len={len(test_ids)}, d_model={d_model})")
    print(f"\n     Wektor dla tokena 0 (pierwsze 8 wartoÅ›ci):")
    print(f"     {[round(v, 3) for v in output[0, 0, :8].tolist()]}")
    print(f"\n     Wektor dla tokena 1 (pierwsze 8 wartoÅ›ci):")
    print(f"     {[round(v, 3) for v in output[0, 1, :8].tolist()]}")
    print(f"\n     â†‘ KaÅ¼dy token to teraz wektor {d_model} liczb")
    print(f"       Model moÅ¼e z nimi liczyÄ‡!")

    quiz(
        "Dlaczego potrzebujemy embeddingu pozycyjnego?",
        [
            "Å»eby model dziaÅ‚aÅ‚ szybciej",
            "Bo attention traktuje tokeny jak ZBIÃ“R, nie zna kolejnoÅ›ci",
            "Å»eby zmniejszyÄ‡ liczbÄ™ parametrÃ³w",
            "Å»eby tekst byÅ‚ krÃ³tszy"
        ],
        2,
        "Attention oblicza podobieÅ„stwo miÄ™dzy KAÅ»DÄ„ parÄ… tokenÃ³w. "
        "Bez pozycji 'kot siedzi na macie' = 'macie na siedzi kot'!"
    )


# ================================================================
#  KROK 4: SELF-ATTENTION (jedna gÅ‚owa)
# ================================================================

class SingleHeadAttention(nn.Module):
    def __init__(self, d_model, d_head, dropout=0.1):
        super().__init__()
        self.W_q = nn.Linear(d_model, d_head, bias=False)
        self.W_k = nn.Linear(d_model, d_head, bias=False)
        self.W_v = nn.Linear(d_model, d_head, bias=False)
        self.dropout = nn.Dropout(dropout)
        self.d_head = d_head

    def forward(self, x, mask=None):
        Q = self.W_q(x)
        K = self.W_k(x)
        V = self.W_v(x)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        return torch.matmul(attn_weights, V), attn_weights


def step_4_single_attention():
    show_header(4, 11, "SELF-ATTENTION (jedna gÅ‚owa)")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SERCE TRANSFORMERA!                                         â•‘
â•‘                                                              â•‘
â•‘  Problem: "Kot siedziaÅ‚ na macie bo BYÅ zmÄ™czony"            â•‘
â•‘  Pytanie: "byÅ‚" odnosi siÄ™ do "kot" czy "maty"?              â•‘
â•‘                                                              â•‘
â•‘  Self-Attention pozwala kaÅ¼demu tokenowi "patrzeÄ‡" na         â•‘
â•‘  inne tokeny i decydowaÄ‡, ktÃ³re sÄ… waÅ¼ne.                    â•‘
â•‘                                                              â•‘
â•‘  MECHANIZM Q-K-V:                                           â•‘
â•‘  KaÅ¼dy token produkuje 3 wektory:                            â•‘
â•‘                                                              â•‘
â•‘  Q (Query/Zapytanie): "Czego szukam?"                        â•‘
â•‘    â†’ Jak pytanie w wyszukiwarce                              â•‘
â•‘                                                              â•‘
â•‘  K (Key/Klucz): "Co oferujÄ™?"                                â•‘
â•‘    â†’ Jak tytuÅ‚ strony                                        â•‘
â•‘                                                              â•‘
â•‘  V (Value/WartoÅ›Ä‡): "JakÄ… informacjÄ™ niosÄ™?"                 â•‘
â•‘    â†’ Jak zawartoÅ›Ä‡ strony                                    â•‘
â•‘                                                              â•‘
â•‘  score = Q Â· K^T     (jak bardzo pasuje pytanie do oferty)   â•‘
â•‘  score = score / âˆšd  (skalowanie, Å¼eby softmax byÅ‚ stabilny) â•‘
â•‘  wagi = softmax(score)  (normalizacja do prawdopodobieÅ„stw)  â•‘
â•‘  wynik = wagi Â· V       (waÅ¼ona suma wartoÅ›ci)               â•‘
â•‘                                                              â•‘
â•‘  MASKA KAUZALNA:                                             â•‘
â•‘  Token na pozycji 3 NIE MOÅ»E patrzeÄ‡ na pozycjÄ™ 4, 5, 6...  â•‘
â•‘  Bo przy generowaniu tych tokenÃ³w jeszcze nie ma!            â•‘
â•‘                                                              â•‘
â•‘  Macierz maski (1=widzi, 0=zablokowane):                     â•‘
â•‘  [[1, 0, 0, 0],                                             â•‘
â•‘   [1, 1, 0, 0],                                             â•‘
â•‘   [1, 1, 1, 0],                                             â•‘
â•‘   [1, 1, 1, 1]]                                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
class SingleHeadAttention(nn.Module):
    def __init__(self, d_model, d_head):
        super().__init__()
        # Projekcje liniowe â€” UCZONE macierze
        self.W_q = nn.Linear(d_model, d_head, bias=False)  # Query
        self.W_k = nn.Linear(d_model, d_head, bias=False)  # Key
        self.W_v = nn.Linear(d_model, d_head, bias=False)  # Value
        self.d_head = d_head

    def forward(self, x, mask=None):
        Q = self.W_q(x)    # [batch, seq_len, d_head]
        K = self.W_k(x)    # [batch, seq_len, d_head]
        V = self.W_v(x)    # [batch, seq_len, d_head]

        # Wyniki attention: kaÅ¼dy token vs kaÅ¼dy token
        scores = Q @ K.transpose(-2, -1)  # [batch, seq_len, seq_len]
        scores = scores / sqrt(d_head)     # skalowanie!

        # Maska: zablokuj przyszÅ‚oÅ›Ä‡
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -infinity)

        weights = softmax(scores, dim=-1)  # prawdopodobieÅ„stwa
        output = weights @ V               # waÅ¼ona suma wartoÅ›ci
        return output
    """)

    wait("Enter â†’ zobacz attention w akcji...")

    show_demo("Self-Attention")

    d_model = 16
    d_head = 8
    seq_len = 4

    attn = SingleHeadAttention(d_model, d_head, dropout=0.0)
    x = torch.randn(1, seq_len, d_model)
    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)

    output, weights = attn(x, mask)

    print(f"\n     WejÅ›cie: {x.shape}  (1 batch, {seq_len} tokeny, {d_model} wymiarÃ³w)")
    print(f"     WyjÅ›cie: {output.shape}  (1 batch, {seq_len} tokeny, {d_head} wymiarÃ³w)")
    print(f"\n     Wagi attention (kto na kogo patrzy):")
    print(f"     Maska kauzalna â†’ tokeny patrzÄ… tylko W TYÅ")
    print()

    w = weights[0].detach()
    labels = ["Tok0", "Tok1", "Tok2", "Tok3"]
    header = "            " + "".join(f"{l:>8}" for l in labels)
    print(f"     {header}")
    for i, label in enumerate(labels):
        row = "     " + f"{label:>10}  "
        for j in range(seq_len):
            v = w[i, j].item()
            if v > 0.3:
                row += f"â–ˆâ–ˆ{v:.2f} "
            elif v > 0.1:
                row += f"â–‘â–‘{v:.2f} "
            else:
                row += f"Â·Â·{v:.2f} "
        print(row)

    print(f"\n     â†‘ Tok0 patrzy TYLKO na siebie (maska!)")
    print(f"       Tok3 patrzy na Tok0, Tok1, Tok2, Tok3")

    quiz(
        "Dlaczego dzielimy scores przez âˆšd_head?",
        [
            "Å»eby model byÅ‚ szybszy",
            "Å»eby softmax nie byÅ‚ zbyt 'ostry' (stabilizacja gradientÃ³w)",
            "Å»eby zmniejszyÄ‡ liczbÄ™ parametrÃ³w",
            "To jest opcjonalne"
        ],
        2,
        "Bez skalowania, iloczyn skalarny roÅ›nie z wymiarem. "
        "DuÅ¼e wartoÅ›ci â†’ softmax daje [0, 0, 1, 0] â†’ gradient zanika. "
        "âˆšd normalizuje skalÄ™."
    )


# ================================================================
#  KROK 5: MULTI-HEAD ATTENTION
# ================================================================

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads

        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=False)
        self.W_o = nn.Linear(d_model, d_model, bias=False)
        self.dropout = nn.Dropout(dropout)
        self.attn_weights = None

    def forward(self, x, mask=None):
        B, T, C = x.shape
        qkv = self.W_qkv(x)
        Q, K, V = qkv.chunk(3, dim=-1)

        Q = Q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)
        K = K.view(B, T, self.n_heads, self.d_head).transpose(1, 2)
        V = V.view(B, T, self.n_heads, self.d_head).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attn = F.softmax(scores, dim=-1)
        self.attn_weights = attn.detach()
        attn = self.dropout(attn)

        context = torch.matmul(attn, V)
        context = context.transpose(1, 2).contiguous().view(B, T, self.d_model)
        return self.W_o(context)


def step_5_multihead():
    show_header(5, 11, "MULTI-HEAD ATTENTION")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  DLACZEGO WIELE GÅÃ“W?                                        â•‘
â•‘                                                              â•‘
â•‘  Jedna gÅ‚owa patrzy na relacje z JEDNEJ perspektywy.         â•‘
â•‘  Ale jÄ™zyk ma WIELE typÃ³w relacji:                            â•‘
â•‘                                                              â•‘
â•‘  GÅ‚owa 1: podmiot â†’ orzeczenie ("Kot" â†’ "siedzi")            â•‘
â•‘  GÅ‚owa 2: przymiotnik â†’ rzeczownik ("duÅ¼y" â†’ "kot")          â•‘
â•‘  GÅ‚owa 3: zaimek â†’ odwoÅ‚anie ("on" â†’ "kot")                  â•‘
â•‘  GÅ‚owa 4: pozycja â†’ wzorzec (lokalne zaleÅ¼noÅ›ci)              â•‘
â•‘                                                              â•‘
â•‘  WYDAJNA IMPLEMENTACJA:                                      â•‘
â•‘  Zamiast 4 osobnych macierzy Q, K, V...                      â•‘
â•‘  ...jedna duÅ¼a macierz + reshape na 4 gÅ‚owy!                 â•‘
â•‘                                                              â•‘
â•‘  Matematycznie identyczne, ale GPU robi to szybciej.         â•‘
â•‘                                                              â•‘
â•‘  PRZEPÅYW KSZTAÅTÃ“W:                                         â•‘
â•‘  [batch, seq, d_model]                                       â•‘
â•‘    â†“ W_qkv (jedna macierz!)                                  â•‘
â•‘  [batch, seq, 3Ã—d_model]                                     â•‘
â•‘    â†“ chunk na Q, K, V                                         â•‘
â•‘  3Ã— [batch, seq, d_model]                                    â•‘
â•‘    â†“ reshape na gÅ‚owy                                         â•‘
â•‘  3Ã— [batch, n_heads, seq, d_head]                            â•‘
â•‘    â†“ attention                                                â•‘
â•‘  [batch, n_heads, seq, d_head]                               â•‘
â•‘    â†“ concat + projekcja                                       â•‘
â•‘  [batch, seq, d_model]                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        self.n_heads = n_heads
        self.d_head = d_model // n_heads

        # JEDNA macierz dla Q, K, V wszystkich gÅ‚Ã³w naraz!
        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=False)
        self.W_o = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x, mask=None):
        B, T, C = x.shape

        # Jedna operacja zamiast trzech
        qkv = self.W_qkv(x)        # [B, T, 3*d_model]
        Q, K, V = qkv.chunk(3, -1)  # 3Ã— [B, T, d_model]

        # Reshape: podziel d_model na n_heads Ã— d_head
        Q = Q.view(B, T, n_heads, d_head).transpose(1, 2)
        # teraz: [B, n_heads, T, d_head]

        # Attention (identycznie jak single-head, ale per gÅ‚owa)
        scores = (Q @ K.T) / sqrt(d_head)
        attn = softmax(scores)
        context = attn @ V

        # ZÅ‚Ã³Å¼ gÅ‚owy z powrotem
        context = context.transpose(1,2).reshape(B, T, d_model)
        return self.W_o(context)  # projekcja wyjÅ›ciowa
    """)

    wait("Enter â†’ zobacz multi-head w akcji...")

    show_demo("Multi-Head Attention (4 gÅ‚owy)")

    d_model = 32
    n_heads = 4
    seq_len = 4

    mha = MultiHeadAttention(d_model, n_heads, dropout=0.0)
    x = torch.randn(1, seq_len, d_model)
    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)

    output = mha(x, mask)

    print(f"\n     WejÅ›cie:  {x.shape}")
    print(f"     WyjÅ›cie:  {output.shape}")
    print(f"     GÅ‚owy:    {n_heads}")
    print(f"     d_head:   {d_model // n_heads} (d_model/n_heads = {d_model}/{n_heads})")
    print(f"\n     Parametry W_qkv: {d_model} Ã— {3*d_model} = {d_model * 3 * d_model:,}")
    print(f"     Parametry W_o:   {d_model} Ã— {d_model} = {d_model * d_model:,}")

    quiz(
        "Dlaczego uÅ¼ywamy jednej macierzy W_qkv zamiast trzech osobnych?",
        [
            "Bo jest dokÅ‚adniejsze",
            "Bo GPU wykonuje jednÄ… duÅ¼Ä… operacjÄ™ szybciej niÅ¼ trzy maÅ‚e",
            "Bo zmniejsza liczbÄ™ parametrÃ³w",
            "Bo jest Å‚atwiejsze do zrozumienia"
        ],
        2,
        "Matematycznie to samo! Ale GPU jest zoptymalizowane do duÅ¼ych "
        "mnoÅ¼eÅ„ macierzy. Jedna operacja 3Ã— wiÄ™ksza > trzy maÅ‚e operacje."
    )


# ================================================================
#  KROK 6: FEED-FORWARD NETWORK
# ================================================================

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()

    def forward(self, x):
        return self.linear2(self.dropout(self.activation(self.linear1(x))))


def step_6_feedforward():
    show_header(6, 11, "FEED-FORWARD NETWORK")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CO TO JEST?                                                 â•‘
â•‘                                                              â•‘
â•‘  Attention ÅÄ„CZY informacje miÄ™dzy tokenami.                 â•‘
â•‘  FFN PRZETWARZA informacje dla kaÅ¼dego tokena z OSOBNA.      â•‘
â•‘                                                              â•‘
â•‘  Analogia:                                                   â•‘
â•‘  Attention = spotkanie zespoÅ‚u (wymiana informacji)           â•‘
â•‘  FFN = kaÅ¼dy pracuje nad swoim zadaniem (przetwarzanie)      â•‘
â•‘                                                              â•‘
â•‘  ARCHITEKTURA:                                               â•‘
â•‘  d_model â†’ d_ff â†’ d_model                                   â•‘
â•‘  64      â†’ 256  â†’ 64                                        â•‘
â•‘  (info)  â†’ (rozszerzenie + GELU) â†’ (kompresja)              â•‘
â•‘                                                              â•‘
â•‘  Dlaczego 4Ã— rozszerzenie?                                   â•‘
â•‘  Daje "przestrzeÅ„ roboczÄ…" do obliczeÅ„.                      â•‘
â•‘  GPT-2: d_model=768, d_ff=3072 (4Ã—)                         â•‘
â•‘                                                              â•‘
â•‘  GELU vs ReLU:                                               â•‘
â•‘  ReLU: max(0, x)   â€” twarde odciÄ™cie __|/                    â•‘
â•‘  GELU: xÂ·Î¦(x)      â€” gÅ‚adkie odciÄ™cie __/â€¾                  â•‘
â•‘  GPT-2, BERT, GPT-3 uÅ¼ywajÄ… GELU (lekko lepsze wyniki)      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)     # rozszerzenie
        self.linear2 = nn.Linear(d_ff, d_model)      # kompresja
        self.activation = nn.GELU()                   # nieliniowoÅ›Ä‡
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.linear1(x)      # d_model â†’ d_ff (rozszerzenie)
        x = self.activation(x)   # GELU
        x = self.dropout(x)
        x = self.linear2(x)      # d_ff â†’ d_model (kompresja)
        return x
    """)

    wait("Enter â†’ zobacz FFN w akcji...")

    show_demo("Feed-Forward Network")

    d_model = 32
    d_ff = 128
    ffn = FeedForward(d_model, d_ff, dropout=0.0)

    x = torch.randn(1, 4, d_model)
    output = ffn(x)

    print(f"\n     WejÅ›cie:     {x.shape}")
    print(f"     Po linear1:  [1, 4, {d_ff}]  (rozszerzenie {d_ff//d_model}Ã—)")
    print(f"     Po GELU:     [1, 4, {d_ff}]  (nieliniowoÅ›Ä‡)")
    print(f"     Po linear2:  {output.shape}  (kompresja)")
    print(f"\n     Parametry: {d_model*d_ff + d_ff*d_model:,} "
          f"({d_model}Ã—{d_ff} + {d_ff}Ã—{d_model})")

    quiz(
        "Co robi FFN czego attention nie robi?",
        [
            "ÅÄ…czy tokeny ze sobÄ…",
            "Przetwarza kaÅ¼dy token NIEZALEÅ»NIE (dodaje 'moc obliczeniowÄ…')",
            "Normalizuje wartoÅ›ci",
            "Zapisuje stan modelu"
        ],
        2,
        "Attention Å‚Ä…czy informacje MIÄ˜DZY tokenami. FFN przetwarza "
        "kaÅ¼dy token z OSOBNA. To jak: attention zbiera informacje "
        "ze spotkania, FFN je przetwarza."
    )


# ================================================================
#  KROK 7: BLOK TRANSFORMERA
# ================================================================

class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        residual = x
        x = self.norm1(x)
        x = self.attn(x, mask)
        x = self.dropout1(x)
        x = residual + x

        residual = x
        x = self.norm2(x)
        x = self.ffn(x)
        x = self.dropout2(x)
        x = residual + x

        return x


def step_7_transformer_block():
    show_header(7, 11, "BLOK TRANSFORMERA")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SkÅ‚adamy attention + FFN w jeden BLOK.                      â•‘
â•‘  Model to STOS takich blokÃ³w (GPT-2: 12 sztuk).             â•‘
â•‘                                                              â•‘
â•‘  ARCHITEKTURA BLOKU (Pre-Norm, jak GPT-2):                   â•‘
â•‘                                                              â•‘
â•‘  WejÅ›cie â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â•‘
â•‘     â†“                                 â”‚                      â•‘
â•‘  LayerNorm                            â”‚                      â•‘
â•‘     â†“                                 â”‚ RESIDUAL             â•‘
â•‘  Multi-Head Attention                 â”‚ CONNECTION           â•‘
â•‘     â†“                                 â”‚ (autostrada          â•‘
â•‘  Dropout                              â”‚  dla gradientÃ³w)     â•‘
â•‘     â†“                                 â”‚                      â•‘
â•‘     + â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â•‘
â•‘     â†“                                                        â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â•‘
â•‘     â†“                                 â”‚                      â•‘
â•‘  LayerNorm                            â”‚                      â•‘
â•‘     â†“                                 â”‚ RESIDUAL             â•‘
â•‘  Feed-Forward                         â”‚ CONNECTION           â•‘
â•‘     â†“                                 â”‚                      â•‘
â•‘  Dropout                              â”‚                      â•‘
â•‘     â†“                                 â”‚                      â•‘
â•‘     + â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â•‘
â•‘     â†“                                                        â•‘
â•‘  WyjÅ›cie                                                     â•‘
â•‘                                                              â•‘
â•‘  RESIDUAL CONNECTION (x + sublayer(x)):                      â•‘
â•‘  "Autostrada" dla gradientÃ³w. Bez tego gÅ‚Ä™bokie              â•‘
â•‘  sieci NIE TRENUJÄ„ SIÄ˜ (vanishing gradients).               â•‘
â•‘                                                              â•‘
â•‘  LAYER NORM:                                                 â•‘
â•‘  Normalizuje aktywacje â†’ stabilny trening.                   â•‘
â•‘  Pre-Norm (przed sublayer) > Post-Norm (po).                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        # Attention z residual connection
        residual = x
        x = self.norm1(x)        # Pre-Norm
        x = self.attn(x, mask)   # Multi-Head Attention
        x = residual + x         # Residual: x + attention(x)

        # FFN z residual connection
        residual = x
        x = self.norm2(x)        # Pre-Norm
        x = self.ffn(x)          # Feed-Forward
        x = residual + x         # Residual: x + ffn(x)

        return x
    """)

    wait("Enter â†’ zobacz blok w akcji...")

    show_demo("Blok Transformera")

    d_model = 32
    block = TransformerBlock(d_model, n_heads=4, d_ff=128, dropout=0.0)
    x = torch.randn(1, 4, d_model)
    mask = torch.tril(torch.ones(4, 4)).unsqueeze(0).unsqueeze(0)

    output = block(x, mask)

    n_params = sum(p.numel() for p in block.parameters())
    print(f"\n     WejÅ›cie:   {x.shape}")
    print(f"     WyjÅ›cie:   {output.shape}  (ten sam ksztaÅ‚t!)")
    print(f"     Parametry: {n_params:,}")
    print(f"\n     SkÅ‚adniki:")
    for name, param in block.named_parameters():
        print(f"       {name}: {list(param.shape)}")

    quiz(
        "Po co jest residual connection (x + sublayer(x))?",
        [
            "Å»eby model byÅ‚ szybszy",
            "Å»eby gradienty mogÅ‚y przepÅ‚ywaÄ‡ bez przeszkÃ³d (autostrada)",
            "Å»eby zmniejszyÄ‡ rozmiar modelu",
            "Å»eby normalizowaÄ‡ dane"
        ],
        2,
        "Bez residual, gradient musi przejÅ›Ä‡ przez KAÅ»DÄ„ warstwÄ™. "
        "Na 12 warstwach zanika do zera. Residual daje 'obejÅ›cie' â€” "
        "gradient przepÅ‚ywa bezpoÅ›rednio. Dlatego gÅ‚Ä™bokie sieci dziaÅ‚ajÄ…!"
    )


# ================================================================
#  KROK 8: PEÅNY MODEL GPT
# ================================================================

class GPTModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embedding = TransformerEmbedding(
            config["vocab_size"], config["d_model"],
            config["max_seq_len"], config["dropout"])

        self.blocks = nn.ModuleList([
            TransformerBlock(config["d_model"], config["n_heads"],
                           config["d_ff"], config["dropout"])
            for _ in range(config["n_layers"])
        ])

        self.final_norm = nn.LayerNorm(config["d_model"])
        self.output_head = nn.Linear(config["d_model"],
                                     config["vocab_size"], bias=False)

        # Weight tying
        self.output_head.weight = self.embedding.token_emb.weight

        self.apply(self._init_weights)
        self.n_params = sum(p.numel() for p in self.parameters())

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)

    def forward(self, x, targets=None):
        seq_len = x.size(1)
        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))
        mask = mask.unsqueeze(0).unsqueeze(0)

        h = self.embedding(x)
        for block in self.blocks:
            h = block(h, mask)
        h = self.final_norm(h)
        logits = self.output_head(h)

        loss = None
        if targets is not None:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1), ignore_index=0)

        return logits, loss

    @torch.no_grad()
    def generate(self, tokenizer, prompt, max_len=200,
                 temperature=0.8, top_k=40, top_p=0.9):
        self.eval()
        device = next(self.parameters()).device
        tokens = tokenizer.encode(prompt)
        if tokens[-1] == tokenizer.vocab[tokenizer.eos_token]:
            tokens = tokens[:-1]
        token_tensor = torch.tensor([tokens], dtype=torch.long, device=device)

        for _ in range(max_len):
            input_tokens = token_tensor[:, -self.config["max_seq_len"]:]
            logits, _ = self(input_tokens)
            next_logits = logits[0, -1, :] / max(temperature, 1e-8)

            if top_k > 0:
                topk_v, _ = torch.topk(next_logits, min(top_k, next_logits.size(0)))
                next_logits[next_logits < topk_v[-1]] = float('-inf')

            if top_p < 1.0:
                sorted_l, sorted_i = torch.sort(next_logits, descending=True)
                cum_p = torch.cumsum(F.softmax(sorted_l, dim=-1), dim=-1)
                mask = cum_p - F.softmax(sorted_l, dim=-1) >= top_p
                sorted_l[mask] = float('-inf')
                next_logits = torch.zeros_like(next_logits).scatter(0, sorted_i, sorted_l)

            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, 1)
            token_tensor = torch.cat([token_tensor, next_token.unsqueeze(0)], dim=1)

            if next_token.item() == tokenizer.vocab[tokenizer.eos_token]:
                break

        return tokenizer.decode(token_tensor[0].tolist())


def step_8_full_model(tokenizer):
    show_header(8, 11, "PEÅNY MODEL GPT")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Teraz skÅ‚adamy WSZYSTKO w jeden model:                      â•‘
â•‘                                                              â•‘
â•‘  ID tokenÃ³w  [batch, seq_len]                                â•‘
â•‘      â†“                                                       â•‘
â•‘  Embedding + Pozycja  [batch, seq_len, d_model]              â•‘
â•‘      â†“                                                       â•‘
â•‘  TransformerBlock 1  (attention + FFN)                        â•‘
â•‘      â†“                                                       â•‘
â•‘  TransformerBlock 2  (attention + FFN)                        â•‘
â•‘      â†“                                                       â•‘
â•‘  TransformerBlock 3  (attention + FFN)                        â•‘
â•‘      â†“                                                       â•‘
â•‘  TransformerBlock 4  (attention + FFN)                        â•‘
â•‘      â†“                                                       â•‘
â•‘  LayerNorm                                                   â•‘
â•‘      â†“                                                       â•‘
â•‘  Linear â†’ logity  [batch, seq_len, vocab_size]               â•‘
â•‘      â†“                                                       â•‘
â•‘  softmax â†’ prawdopodobieÅ„stwa nastÄ™pnego tokena              â•‘
â•‘                                                              â•‘
â•‘  WEIGHT TYING (wiÄ…zanie wag):                                â•‘
â•‘  Macierz embeddingÃ³w = macierz wyjÅ›ciowa (ta sama!)          â•‘
â•‘  To samo koduje tokeny CO dekoduje predykcje.                â•‘
â•‘  Redukuje parametry, poprawia generalizacjÄ™.                 â•‘
â•‘  UÅ¼ywane w GPT-2, GPT-3, LLaMA.                             â•‘
â•‘                                                              â•‘
â•‘  INICJALIZACJA WAG:                                          â•‘
â•‘  Normal(0, 0.02) â€” standard GPT-2.                           â•‘
â•‘  ZÅ‚e inicjalizacje = model siÄ™ nie uczy!                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
class GPTModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embedding = TransformerEmbedding(
            config["vocab_size"], config["d_model"],
            config["max_seq_len"])

        # Stos N blokÃ³w transformera
        self.blocks = nn.ModuleList([
            TransformerBlock(config["d_model"], config["n_heads"],
                           config["d_ff"])
            for _ in range(config["n_layers"])
        ])

        self.final_norm = nn.LayerNorm(config["d_model"])

        # Projekcja na sÅ‚ownik: d_model â†’ vocab_size
        self.output_head = nn.Linear(config["d_model"], config["vocab_size"])

        # Weight tying!
        self.output_head.weight = self.embedding.token_emb.weight

    def forward(self, x, targets=None):
        mask = causal_mask(seq_len)     # dolnotrÃ³jkÄ…tna
        h = self.embedding(x)           # tokeny â†’ wektory
        for block in self.blocks:
            h = block(h, mask)           # N Ã— (attention + FFN)
        h = self.final_norm(h)           # koÅ„cowa normalizacja
        logits = self.output_head(h)     # wektory â†’ logity

        # Loss: cross-entropy miÄ™dzy predykcjami a celami
        if targets is not None:
            loss = cross_entropy(logits, targets)
        return logits, loss
    """)

    wait("Enter â†’ zobacz model w akcji...")

    show_demo("PeÅ‚ny model GPT")

    config = dict(DEFAULT_CONFIG)
    config["vocab_size"] = len(tokenizer.vocab) 

    model = GPTModel(config)

    print(f"\n     ğŸ“Š ARCHITEKTURA:")
    print(f"     Warstwy:     {config['n_layers']}")
    print(f"     GÅ‚owy:       {config['n_heads']}")
    print(f"     d_model:     {config['d_model']}")
    print(f"     d_ff:        {config['d_ff']}")
    print(f"     SÅ‚ownik:     {config['vocab_size']}")
    print(f"     Kontekst:    {config['max_seq_len']}")
    print(f"     Parametry:   {model.n_params:,}")

    test_ids = tokenizer.encode("Kot siedzi na")
    x = torch.tensor([test_ids])
    logits, _ = model(x)

    probs = F.softmax(logits[0, -1, :], dim=-1)
    top5 = torch.topk(probs, 5)
    print(f"\n     ğŸ¯ Predykcje (PRZED treningiem â€” losowe!):")
    for p, idx in zip(top5.values, top5.indices):
        word = tokenizer.inverse_vocab.get(idx.item(), '?')
        print(f"        {p.item():.3f} â†’ '{word}'")

    print(f"\n     â†‘ Losowe predykcje â€” model jeszcze nic nie wie!")
    print(f"       Po treningu bÄ™dÄ… sensowne.")

    quiz(
        "Co to jest weight tying?",
        [
            "WiÄ…zanie learning rate z liczbÄ… epok",
            "Ta sama macierz koduje tokeny (embedding) i dekoduje predykcje (output)",
            "ZamraÅ¼anie wag podczas treningu",
            "Kopiowanie wag miÄ™dzy warstwami"
        ],
        2,
        "Embedding (vocabâ†’d_model) i output head (d_modelâ†’vocab) to ta sama "
        "macierz! Logicznie: jeÅ›li 'kot' koduje siÄ™ jako wektor X, to wektor "
        "X powinien dekodowaÄ‡ siÄ™ z powrotem na 'kot'."
    )

    return model, config


# ================================================================
#  KROK 9: PÄ˜TLA TRENINGOWA
# ================================================================

def step_9_training(model, tokenizer, text, config):
    show_header(9, 11, "PÄ˜TLA TRENINGOWA")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Teraz UCZYMY model â€” to serce caÅ‚ego procesu.               â•‘
â•‘                                                              â•‘
â•‘  PÄ˜TLA TRENINGOWA:                                           â•‘
â•‘  1. Forward:  model(wejÅ›cie) â†’ predykcje                     â•‘
â•‘  2. Loss:     CrossEntropy(predykcje, cel)                   â•‘
â•‘  3. Backward: oblicz gradienty (âˆ‚loss/âˆ‚wagi)                 â•‘
â•‘  4. Update:   wagi -= lr Ã— gradienty                         â•‘
â•‘  5. PowtÃ³rz                                                  â•‘
â•‘                                                              â•‘
â•‘  OPTYMALIZATOR AdamW:                                        â•‘
â•‘  Adam + poprawiony weight decay. Standard w LLM.             â•‘
â•‘  KaÅ¼da waga ma SWÃ“J adaptacyjny learning rate.               â•‘
â•‘                                                              â•‘
â•‘  HARMONOGRAM LR (warmup + zanik cosinusowy):                 â•‘
â•‘  lr â”‚     /â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾\                                      â•‘
â•‘     â”‚    /              \                                     â•‘
â•‘     â”‚   /                \                                    â•‘
â•‘     â”‚  /                  \                                   â•‘
â•‘     â”‚ /                    \_____                              â•‘
â•‘     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ kroki                      â•‘
â•‘      warmup    cosine decay                                   â•‘
â•‘                                                              â•‘
â•‘  GRADIENT CLIPPING:                                          â•‘
â•‘  Obcina gradienty > 1.0. Zapobiega eksplozji.                â•‘
â•‘                                                              â•‘
â•‘  PERPLEXITY = e^loss:                                        â•‘
â•‘  "Ile tokenÃ³w model waha siÄ™ miÄ™dzy"                         â•‘
â•‘  PPL=1: idealny. PPL=vocab_size: losowe zgadywanie.          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    show_code("""
# Optymalizator (nie stosuj weight decay do bias i norm!)
optimizer = torch.optim.AdamW([
    {"params": decay_params, "weight_decay": 0.01},
    {"params": no_decay_params, "weight_decay": 0.0},
], lr=3e-4)

for epoch in range(n_epochs):
    for batch_x, batch_y in dataloader:
        # 1. Forward
        logits, loss = model(batch_x, targets=batch_y)

        # 2. Backward
        optimizer.zero_grad()
        loss.backward()

        # 3. Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # 4. Update wag
        optimizer.step()

        # 5. Aktualizuj learning rate (warmup + cosine)
    """)

    wait("Enter â†’ trenujemy model! (to potrwa kilka sekund)...")

    show_demo("Trening modelu")

    clean_text = re.sub(r'\s+', ' ', text).strip()
    token_ids = tokenizer.encode(clean_text)

    seq_len = min(64, config["max_seq_len"])
    batch_size = min(8, config["batch_size"])
    epochs = 15

    dataset = TextDataset(token_ids, seq_len)
    loader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=True, drop_last=True)

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)

    model.train()
    start = time.time()

    print(f"\n     TokenÃ³w: {len(token_ids):,}")
    print(f"     PrzykÅ‚adÃ³w: {len(dataset):,}")
    print(f"     Batchy: {len(loader)}")
    print(f"     Epoki: {epochs}")
    print(f"     {'â”€'*50}")

    for epoch in range(epochs):
        total_loss = 0
        n = 0
        for bx, by in loader:
            logits, loss = model(bx, targets=by)
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item()
            n += 1

        avg_loss = total_loss / max(n, 1)
        ppl = math.exp(avg_loss) if avg_loss < 20 else float('inf')

        if (epoch + 1) % 3 == 0 or epoch == 0:
            elapsed = time.time() - start
            bar = "â–ˆ" * int(30 * (1 - min(avg_loss / 6, 1)))
            bar += "â–‘" * (30 - len(bar))
            print(f"     Epoka {epoch+1:3d}/{epochs} â”‚ "
                  f"Loss: {avg_loss:.4f} â”‚ PPL: {ppl:8.1f} â”‚ "
                  f"[{bar}] â”‚ {elapsed:.0f}s")

    elapsed = time.time() - start
    print(f"     {'â”€'*50}")
    print(f"     âœ… Gotowe w {elapsed:.1f}s! Loss: {avg_loss:.4f}")

    # PokaÅ¼ predykcje PO treningu
    model.eval()
    test_ids = tokenizer.encode("Kot siedzi na")
    x = torch.tensor([test_ids])
    logits, _ = model(x)

    probs = F.softmax(logits[0, -1, :], dim=-1)
    top5 = torch.topk(probs, 5)
    print(f"\n     ğŸ¯ Predykcje PO treningu ('Kot siedzi na' â†’ ?):")
    for p, idx in zip(top5.values, top5.indices):
        word = tokenizer.inverse_vocab.get(idx.item(), '?')
        print(f"        {p.item():.3f} â†’ '{word}'")

    quiz(
        "Dlaczego stosujemy warmup learning rate?",
        [
            "Å»eby model zapomniaÅ‚ poprzedni trening",
            "Bo na poczÄ…tku wagi sÄ… losowe â€” duÅ¼y LR + losowe wagi = chaos",
            "Å»eby przyspieszyÄ‡ trening",
            "Å»eby zmniejszyÄ‡ overfitting"
        ],
        2,
        "Losowe wagi + duÅ¼y LR = ogromne aktualizacje w losowych kierunkach. "
        "Warmup: zaczynamy od malutkich krokÃ³w, dajemy modelowi siÄ™ 'ustabilizowaÄ‡'."
    )


# ================================================================
#  KROK 10: GENEROWANIE TEKSTU
# ================================================================

def step_10_generation(model, tokenizer, config):
    show_header(10, 11, "GENEROWANIE TEKSTU")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Teraz model GENERUJE tekst â€” token po tokenie.              â•‘
â•‘                                                              â•‘
â•‘  PROCES AUTOREGRESYJNY:                                      â•‘
â•‘  "Kot" â†’ model â†’ P(siedzi)=0.4, P(lubi)=0.3, P(je)=0.2     â•‘
â•‘  â†’ samplingujemy â†’ "siedzi"                                  â•‘
â•‘  "Kot siedzi" â†’ model â†’ P(na)=0.7, P(i)=0.2                â•‘
â•‘  â†’ samplingujemy â†’ "na"                                      â•‘
â•‘  "Kot siedzi na" â†’ model â†’ P(macie)=0.5, P(kanapie)=0.3    â•‘
â•‘  â†’ samplingujemy â†’ "macie"                                   â•‘
â•‘                                                              â•‘
â•‘  STRATEGIE SAMPLINGU:                                        â•‘
â•‘                                                              â•‘
â•‘  1. TEMPERATURA:                                             â•‘
â•‘     Dzieli logity przed softmaxem.                           â•‘
â•‘     temp=0.1: bardzo pewny siebie (powtarzalny)              â•‘
â•‘     temp=1.0: standardowy                                    â•‘
â•‘     temp=2.0: kreatywny chaos                                â•‘
â•‘                                                              â•‘
â•‘  2. TOP-K:                                                   â•‘
â•‘     RozwaÅ¼a tylko K najlepszych tokenÃ³w.                     â•‘
â•‘     top_k=1: greedy (zawsze najlepszy)                       â•‘
â•‘     top_k=40: standard GPT-2                                â•‘
â•‘                                                              â•‘
â•‘  3. TOP-P (Nucleus Sampling):                                â•‘
â•‘     Bierze najmniejszy zbiÃ³r tokenÃ³w o Å‚Ä…cznym P>0.9.        â•‘
â•‘     Adaptacyjne: wiÄ™cej opcji gdy niepewny.                   â•‘
â•‘     UÅ¼ywane w ChatGPT, Claude, Gemini.                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    wait("Enter â†’ generujemy tekst z rÃ³Å¼nymi ustawieniami...")

    show_demo("Generowanie tekstu")

    model.eval()
    prompts = ["Kot siedzi", "Stary czÅ‚owiek", "SÅ‚oÅ„ce"]

    for temp_label, temp in [("Niska (0.3)", 0.3), ("Normalna (0.8)", 0.8),
                              ("Wysoka (1.5)", 1.5)]:
        print(f"\n     ğŸŒ¡ï¸ Temperatura: {temp_label}")
        for prompt in prompts:
            try:
                result = model.generate(tokenizer, prompt, max_len=30,
                                       temperature=temp, top_k=40, top_p=0.9)
                print(f"       '{prompt}' â†’ {result}")
            except Exception as e:
                print(f"       '{prompt}' â†’ BÅ‚Ä…d: {e}")

    quiz(
        "Co robi top-p (nucleus sampling)?",
        [
            "Bierze p procent sÅ‚ownika",
            "Bierze NAJMNIEJSZY zbiÃ³r tokenÃ³w o Å‚Ä…cznym prawdopodobieÅ„stwie â‰¥ p",
            "Sortuje tokeny po dÅ‚ugoÅ›ci",
            "Wybiera p-ty token z listy"
        ],
        2,
        "Top-p jest ADAPTACYJNE. Gdy model jest pewny (jeden token ma 0.9), "
        "rozwaÅ¼a 1-2 tokeny. Gdy niepewny (rozkÅ‚ad pÅ‚aski), rozwaÅ¼a wiele. "
        "Dlatego ChatGPT go uÅ¼ywa!"
    )


# ================================================================
#  KROK 11: TWÃ“J WÅASNY MODEL
# ================================================================

def step_11_your_model(tokenizer, config):
    show_header(11, 11, "TWÃ“J WÅASNY MODEL!")

    show_explanation("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘  ğŸ† GRATULACJE! PrzeszedÅ‚eÅ› caÅ‚y kurs!                      â•‘
â•‘                                                              â•‘
â•‘  Teraz WIESZ jak zbudowaÄ‡ Transformer od zera:               â•‘
â•‘                                                              â•‘
â•‘  âœ… Krok 1:  Tokenizer BPE (tekst â†’ liczby)                  â•‘
â•‘  âœ… Krok 2:  Pipeline danych (next-token prediction)          â•‘
â•‘  âœ… Krok 3:  Embeddingi (token + pozycja)                     â•‘
â•‘  âœ… Krok 4:  Self-Attention (Q, K, V, maska)                  â•‘
â•‘  âœ… Krok 5:  Multi-Head (wiele perspektyw)                    â•‘
â•‘  âœ… Krok 6:  Feed-Forward (przetwarzanie per token)           â•‘
â•‘  âœ… Krok 7:  Blok Transformera (residual + norm)              â•‘
â•‘  âœ… Krok 8:  PeÅ‚ny model GPT (stos blokÃ³w + weight tying)    â•‘
â•‘  âœ… Krok 9:  Trening (AdamW, warmup, clipping)               â•‘
â•‘  âœ… Krok 10: Generowanie (temperatura, top-k, top-p)         â•‘
â•‘                                                              â•‘
â•‘  To jest DOKÅADNIE ta sama architektura co GPT-2/3/4!        â•‘
â•‘  RÃ³Å¼nica: skala (parametry, dane, compute).                  â•‘
â•‘                                                              â•‘
â•‘  CO DALEJ:                                                   â•‘
â•‘  â†’ Teraz moÅ¼esz trenowaÄ‡ na SWOIM tekÅ›cie                    â•‘
â•‘  â†’ Wklej artykuÅ‚ z Wikipedii, ksiÄ…Å¼kÄ™, cokolwiek            â•‘
â•‘  â†’ Eksperymentuj z parametrami                               â•‘
â•‘                                                              â•‘
â•‘  UÅ»YCIE:                                                     â•‘
â•‘  python write_transformer.py --train twoj_plik.txt           â•‘
â•‘  python write_transformer.py --paste                         â•‘
â•‘  python write_transformer.py --interactive                   â•‘
â•‘                                                              â•‘
â•‘  NASTÄ˜PNE KROKI:                                             â•‘
â•‘  â†’ nanoGPT (Karpathy) â€” peÅ‚ny GPT-2                         â•‘
â•‘  â†’ "Attention Is All You Need" â€” oryginalny paper            â•‘
â•‘  â†’ Hugging Face â€” produkcyjne modele                         â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    wait("Enter â†’ tryb interaktywny (generuj co chcesz)...")


# ================================================================
#  TRYB KURSU
# ================================================================

def run_course():
    """GÅ‚Ã³wny kurs krok po kroku."""
    print(f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                              â•‘
    â•‘   ğŸ§  TRANSFORMER OD ZERA â€” KURS KROK PO KROKU               â•‘
    â•‘                                                              â•‘
    â•‘   11 krokÃ³w. Po ukoÅ„czeniu bÄ™dziesz umiaÅ‚                    â•‘
    â•‘   sam napisaÄ‡ i uruchomiÄ‡ Transformer / GPT.                 â•‘
    â•‘                                                              â•‘
    â•‘   KaÅ¼dy krok:                                                â•‘
    â•‘   ğŸ“– WyjaÅ›nienie â€” co i dlaczego                             â•‘
    â•‘   ğŸ“ Kod â€” gotowy, do przepisania                            â•‘
    â•‘   ğŸ”¬ Demo â€” uruchomienie na Å¼ywo                             â•‘
    â•‘   â“ Quiz â€” sprawdzenie zrozumienia                          â•‘
    â•‘                                                              â•‘
    â•‘   Czas: ~30 minut                                            â•‘
    â•‘                                                              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    wait("Enter â†’ zaczynamy!")

    text = DEFAULT_TEXT

    # Krok 1: Tokenizer
    tokenizer = step_1_tokenizer(text)
    wait("Enter â†’ Krok 2: Pipeline danych...")

    # Krok 2: Data pipeline
    token_ids = step_2_data_pipeline(tokenizer, text)
    wait("Enter â†’ Krok 3: Embeddingi...")

    # Krok 3: Embeddings
    step_3_embeddings(tokenizer)
    wait("Enter â†’ Krok 4: Self-Attention...")

    # Krok 4: Single-head attention
    step_4_single_attention()
    wait("Enter â†’ Krok 5: Multi-Head Attention...")

    # Krok 5: Multi-head attention
    step_5_multihead()
    wait("Enter â†’ Krok 6: Feed-Forward Network...")

    # Krok 6: Feed-forward
    step_6_feedforward()
    wait("Enter â†’ Krok 7: Blok Transformera...")

    # Krok 7: Transformer block
    step_7_transformer_block()
    wait("Enter â†’ Krok 8: PeÅ‚ny model GPT...")

    # Krok 8: Full model
    model, config = step_8_full_model(tokenizer)
    wait("Enter â†’ Krok 9: Trening (kilka sekund)...")

    # Krok 9: Training
    step_9_training(model, tokenizer, text, config)
    wait("Enter â†’ Krok 10: Generowanie tekstu...")

    # Krok 10: Generation
    step_10_generation(model, tokenizer, config)
    wait("Enter â†’ Krok 11: Podsumowanie...")

    # Krok 11: Summary
    step_11_your_model(tokenizer, config)

    # Tryb interaktywny
    interactive_mode(model, tokenizer, config)


# ================================================================
#  TRYBY BEZPOÅšREDNIE (train, interactive, paste)
# ================================================================

def run_training_direct(text, config, device='cpu', save_path="checkpoint"):
    """Pipeline treningowy (bez kursu)."""
    print(f"\n  ğŸ§  Trening Transformera...")

    text = re.sub(r'\s+', ' ', text).strip()
    tokenizer = BPETokenizer(config["bpe_vocab_size"])
    tokenizer.train(text)
    config["vocab_size"] = len(tokenizer.vocab)

    token_ids = tokenizer.encode(text)
    print(f"  TokenÃ³w: {len(token_ids):,}, SÅ‚ownik: {config['vocab_size']}")

    split = int(len(token_ids) * 0.9)
    train_loader = torch.utils.data.DataLoader(
        TextDataset(token_ids[:split], config["max_seq_len"]),
        batch_size=config["batch_size"], shuffle=True, drop_last=True)

    model = GPTModel(config)
    print(f"  Model: {model.n_params:,} parametrÃ³w")

    optimizer = torch.optim.AdamW(model.parameters(), lr=config["lr"])

    model.train()
    start = time.time()
    for epoch in range(config["epochs"]):
        total_loss = n = 0
        for bx, by in train_loader:
            _, loss = model(bx, targets=by)
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config["grad_clip"])
            optimizer.step()
            total_loss += loss.item()
            n += 1
        avg = total_loss / max(n, 1)
        if (epoch+1) % max(config["epochs"]//10, 1) == 0 or epoch == 0:
            print(f"  Epoka {epoch+1}/{config['epochs']} â”‚ Loss: {avg:.4f} â”‚ "
                  f"PPL: {math.exp(avg) if avg<20 else float('inf'):.1f} â”‚ "
                  f"{time.time()-start:.0f}s")

    print(f"  âœ… Gotowe w {time.time()-start:.1f}s")

    os.makedirs(save_path, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(save_path, "model.pt"))
    tokenizer.save(os.path.join(save_path, "tokenizer.json"))
    with open(os.path.join(save_path, "config.json"), 'w') as f:
        json.dump(config, f, indent=2)
    print(f"  ğŸ’¾ Zapisano: {save_path}/")

    return model, tokenizer


def interactive_mode(model=None, tokenizer=None, config=None, device='cpu'):
    """Tryb interaktywny."""
    if model is None:
        try:
            with open("checkpoint/config.json") as f:
                config = json.load(f)
            tokenizer = BPETokenizer(config["bpe_vocab_size"])
            tokenizer.load("checkpoint/tokenizer.json")
            config["vocab_size"] = len(tokenizer.vocab)
            model = GPTModel(config)
            model.load_state_dict(torch.load("checkpoint/model.pt",
                                             weights_only=True))
            model.eval()
            print(f"  ğŸ“‚ Wczytano model ({model.n_params:,} parametrÃ³w)")
        except FileNotFoundError:
            print("  âŒ Brak checkpointu. Najpierw wytrenuj model.")
            return

    temp = config.get("temperature", 0.8)
    top_k_val = config.get("top_k", 40)

    print(f"\n  ğŸ® Tryb interaktywny. Wpisz prompt.")
    print(f"  Komendy: /temp 0.5 | /topk 20 | /quit\n")

    while True:
        try:
            prompt = input(f"  ğŸ“ [temp={temp}] > ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n  ğŸ‘‹ Do zobaczenia!")
            break

        if not prompt:
            continue
        if prompt == "/quit":
            break
        if prompt.startswith("/temp "):
            try:
                temp = float(prompt.split()[1])
                print(f"  âœ… Temperatura: {temp}")
            except:
                pass
            continue
        if prompt.startswith("/topk "):
            try:
                top_k_val = int(prompt.split()[1])
                print(f"  âœ… Top-K: {top_k_val}")
            except:
                pass
            continue

        try:
            model.eval()
            result = model.generate(tokenizer, prompt, max_len=100,
                                   temperature=temp, top_k=top_k_val)
            print(f"  â†’ {result}\n")
        except Exception as e:
            print(f"  âŒ {e}")


# ================================================================
#  MAIN
# ================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Transformer od Zera â€” Kurs + NarzÄ™dzie")
    parser.add_argument("--train", type=str, metavar="PLIK",
                        help="Trenuj na pliku (pomija kurs)")
    parser.add_argument("--paste", action="store_true",
                        help="Wklej tekst do treningu")
    parser.add_argument("--interactive", action="store_true",
                        help="Tryb interaktywny")
    parser.add_argument("--generate", type=str, metavar="PROMPT")
    parser.add_argument("--epochs", type=int, default=DEFAULT_CONFIG["epochs"])
    parser.add_argument("--d_model", type=int, default=DEFAULT_CONFIG["d_model"])
    parser.add_argument("--n_heads", type=int, default=DEFAULT_CONFIG["n_heads"])
    parser.add_argument("--n_layers", type=int, default=DEFAULT_CONFIG["n_layers"])
    parser.add_argument("--d_ff", type=int, default=DEFAULT_CONFIG["d_ff"])
    parser.add_argument("--lr", type=float, default=DEFAULT_CONFIG["lr"])
    parser.add_argument("--bpe_vocab_size", type=int,
                        default=DEFAULT_CONFIG["bpe_vocab_size"])

    args = parser.parse_args()

    config = dict(DEFAULT_CONFIG)
    for k in ["epochs", "d_model", "n_heads", "n_layers", "d_ff",
              "lr", "bpe_vocab_size"]:
        config[k] = getattr(args, k)

    if args.interactive:
        interactive_mode()
    elif args.generate:
        interactive_mode()  # wczyta checkpoint i wygeneruje
    elif args.train:
        try:
            with open(args.train, 'r', encoding='utf-8') as f:
                text = f.read()
            model, tok = run_training_direct(text, config)
            interactive_mode(model, tok, config)
        except FileNotFoundError:
            print(f"  âŒ Nie znaleziono: {args.train}")
    elif args.paste:
        print("  Wklej tekst (Ctrl+D / Ctrl+Z gdy gotowe):")
        lines = []
        try:
            while True:
                lines.append(input())
        except EOFError:
            pass
        text = '\n'.join(lines)
        if len(text.strip()) < 100:
            text = DEFAULT_TEXT
        model, tok = run_training_direct(text, config)
        interactive_mode(model, tok, config)
    else:
        # DOMYÅšLNIE: kurs krok po kroku!
        run_course()


if __name__ == "__main__":
    main()