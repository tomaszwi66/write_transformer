<div align="center">

# ğŸ§  write-transformer

**Zbuduj GPT od zera â€” krok po kroku, w 11 lekcjach.**

Interaktywny kurs. Napisz kaÅ¼dy komponent samodzielnie. Trenuj i generuj tekst w kilka minut.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![English Version](https://img.shields.io/badge/Version-English-blue.svg)](README.md)
[![Licencja](https://img.shields.io/badge/Licencja-MIT-green.svg)](LICENSE)

[Szybki Start](#-szybki-start) â€¢ [Architektura](#-architektura) â€¢ [FAQ](#-faq)
<br>
**[ğŸ‡¬ğŸ‡§ English version: `python write_transformer.py`](README.md)**

</div>

---

## ğŸ¯ Dla kogo to jest?

- **PoczÄ…tkujÄ…cy** â€“ chcesz zrozumieÄ‡, jak GPT/ChatGPT naprawdÄ™ dziaÅ‚a "pod maskÄ…".
- **Studenci** â€“ uczysz siÄ™ ML i chcesz samodzielnie zbudowaÄ‡ Transformer.
- **Praktycy** â€“ szukasz czystej, skomentowanej implementacji referencyjnej.

Brak wymagaÅ„ wstÄ™pnych. KaÅ¼dy koncept wyjaÅ›niony za pomocÄ… analogii, kodu, dem i quizÃ³w.

---

## âš¡ Szybki Start

```bash
pip install torch numpy
git clone [https://github.com/tomaszwi66/write-transformer.git](https://github.com/tomaszwi66/write-transformer.git)
cd write-transformer
python write_transformer_PL.py
```

To wszystko. Interaktywny kurs wystartuje automatycznie.

---

## ğŸ“– Jak to dziaÅ‚a?

KaÅ¼dy z 11 krokÃ³w opiera siÄ™ na tym samym schemacie:

* ğŸ“– **WyjaÅ›nienie** â€“ co to jest, dlaczego jest waÅ¼ne, analogia z Å¼ycia.
* ğŸ“ **Kod** â€“ skomentowany, gotowy do samodzielnego wpisania.
* ğŸ”¬ **Demo** â€“ uruchamia dany komponent na Å¼ywo i pokazuje wynik.
* â“ **Quiz** â€“ sprawdza Twoje zrozumienie tematu.
* â **Enter** â€“ przejÅ›cie do nastÄ™pnego kroku.

Nie czytasz podrÄ™cznika â€“ budujesz dziaÅ‚ajÄ…ce GPT element po elemencie. 
Na koÅ„cu wytrenujesz model na tekÅ›cie i wygenerujesz wÅ‚asne zdania.

---

## ğŸ“š 11 KrokÃ³w

| Krok | Temat | Co budujesz? |
| :---: | :--- | :--- |
| **1** | Tokenizer BPE | Tekst â†’ liczby (ten sam algorytm co w GPT-2) |
| **2** | Pipeline danych | Okno przesuwne, przygotowanie do przewidywania tokenÃ³w |
| **3** | Embeddingi | Embeddingi tokenÃ³w i pozycji |
| **4** | Self-Attention | Mechanizm uwagi z maskowaniem przyczynowym |
| **5** | Multi-Head Attention | Wiele perspektyw jednoczeÅ›nie, efektywna implementacja |
| **6** | SieÄ‡ Feed-Forward | Przetwarzanie cech z aktywacjÄ… GELU |
| **7** | Blok Transformera | PoÅ‚Ä…czenia rezydualne + LayerNorm |
| **8** | PeÅ‚ny Model GPT | Stos blokÃ³w + wiÄ…zanie wag (weight tying) |
| **9** | PÄ™tla Treningowa | AdamW, clipping gradientÃ³w, Å›ledzenie perplexity |
| **10** | Generowanie Tekstu | Temperature, top-k, top-p (nucleus sampling) |
| **11** | TwÃ³j WÅ‚asny Model | Podsumowanie + interaktywny plac zabaw |

**PrzykÅ‚adowy wynik z Kroku 9 (Trening):**
```text
ğŸ”¬ DEMO: Trenowanie modelu
Tokeny: 1,847  PrzykÅ‚ady: 1,783  Batche: 222
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Epoch  1/15 â”‚ Loss: 5.2341 â”‚ PPL: 188.0 â”‚ [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] â”‚ 2s
Epoch  3/15 â”‚ Loss: 4.1023 â”‚ PPL:  60.5 â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] â”‚ 5s
Epoch  6/15 â”‚ Loss: 3.2156 â”‚ PPL:  24.9 â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] â”‚ 9s
Epoch 15/15 â”‚ Loss: 2.1847 â”‚ PPL:   8.9 â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] â”‚ 21s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Gotowe w 21.3s! Loss: 2.1847
```

**PrzykÅ‚adowy wynik z Kroku 10 (Generowanie):**
```text
ğŸŒ¡ï¸ Temperatura: Niska (0.3)
'Kot siedzi' â†’ Kot siedzi na macie i obserwuje ptaki za oknem
'Stary czÅ‚owiek' â†’ Stary czÅ‚owiek czyta ksiÄ…Å¼kÄ™ w cichej bibliotece

ğŸŒ¡ï¸ Temperatura: Wysoka (1.5)
'Kot siedzi' â†’ Kot siedzi radoÅ›nie kominku piÄ™kne warzywa
'Stary czÅ‚owiek' â†’ Stary czÅ‚owiek Å‚agodnie kwitnÄ… gwiazdy melodie
```

---

## ğŸš€ Tryby bezpoÅ›rednie

PomiÅ„ kurs i przejdÅº prosto do trenowania lub generowania:

```bash
# Trenuj na wÅ‚asnym pliku tekstowym
python write_transformer_PL.py --train moj_tekst.txt

# Trenuj na wklejonym tekÅ›cie (zakoÅ„cz przez Ctrl+D)
python write_transformer_PL.py --paste

# Interaktywne generowanie (wymaga wytrenowanego modelu)
python write_transformer_PL.py --interactive

# WÅ‚asna architektura
python write_transformer_PL.py --train data.txt --d_model 128 --n_heads 8 --n_layers 6 --epochs 50
```

### Opcje CLI

| Flaga | DomyÅ›lnie | Opis |
| :--- | :---: | :--- |
| `--train PLIK` | â€” | Trenuj na pliku tekstowym |
| `--paste` | â€” | Wklej tekst do treningu |
| `--interactive` | â€” | Interaktywny tryb generowania |
| `--epochs` | 30 | Liczba epok treningowych |
| `--d_model` | 64 | Wymiar embeddingÃ³w |
| `--n_heads` | 4 | Liczba gÅ‚owic uwagi |
| `--n_layers` | 4 | Liczba blokÃ³w transformera |
| `--d_ff` | 256 | Wymiar ukryty sieci Feed-forward |
| `--lr` | 3e-4 | WspÃ³Å‚czynnik uczenia (learning rate) |
| `--bpe_vocab_size`| 512 | Rozmiar sÅ‚ownika BPE |

---

## ğŸ—ï¸ Architektura

Ten projekt implementuje dokÅ‚adnie tÄ™ samÄ… architekturÄ™ co GPT-2/3/4. JedynÄ… rÃ³Å¼nicÄ… jest skala.



```text
                  write-transformer     GPT-2 Small        GPT-4
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SÅ‚ownik           ~512 tokenÃ³w          50,257 tokenÃ³w     ~100K tokenÃ³w
Wymiary           64 wymiary            768 wymiarÃ³w       ~12,288 wymiarÃ³w
GÅ‚owice           4 gÅ‚owice             12 gÅ‚owic          ~96 gÅ‚owic
Warstwy           4 warstwy             12 warstw          ~120 warstw
Parametry         ~50K parametrÃ³w       124M parametrÃ³w    ~1.8T parametrÃ³w
Dane              25 zdaÅ„               8M stron www       caÅ‚y internet
Czas treningu     sekundy na CPU        godziny na 8 GPU   miesiÄ…ce na 25K GPU
```

### Kluczowe decyzje projektowe (zgodne z GPT-2)

| Funkcja | Opis |
| :--- | :--- |
| **Pre-Norm** | LayerNorm przed (a nie po) kaÅ¼dÄ… podwarstwÄ… |
| **Weight Tying** | Macierz embeddingÃ³w = macierz wyjÅ›ciowa |
| **GELU** | GÅ‚adka funkcja aktywacji (zamiast ReLU) |
| **Wyuczone pozycje**| Embeddingi pozycyjne sÄ… uczone, nie sinusoidalne |
| **Maska przyczynowa**| DolnotrÃ³jkÄ…tna â€” tokeny widzÄ… tylko przeszÅ‚oÅ›Ä‡ |
| **AdamW** | Adam z odseparowanym spadkiem wag (weight decay) |

---

## ğŸ§ª Eksperymenty do wyprÃ³bowania

### Åatwe
- PrzejdÅº caÅ‚y kurs (tryb 0) â€” zrozum kaÅ¼dy element.
- ZmieÅ„ temperaturÄ™ w trybie interaktywnym â€” porÃ³wnaj wyniki.
- Trenuj model na zupeÅ‚nie innym pliku tekstowym.

### Åšrednie
- Zmniejsz `d_model` do 8 â€” czy model nadal jest w stanie siÄ™ uczyÄ‡?
- ZwiÄ™ksz `epochs` do 100 â€” czy model zacznie "przeuczaÄ‡" (overfitting)?
- PorÃ³wnaj `n_heads=1` z `n_heads=8` â€” co siÄ™ zmienia w jakoÅ›ci?

### Zaawansowane
- WyÅ‚Ä…cz embeddingi pozycyjne â€” co siÄ™ zepsuje?
- UsuÅ„ poÅ‚Ä…czenia rezydualne â€” czy model nadal bÄ™dzie siÄ™ trenowaÄ‡?
- ZmieÅ„ `bpe_vocab_size` â€” jaki ma wpÅ‚yw na kompresjÄ™ tekstu?

---

## ğŸ“ Struktura Projektu

```text
write-transformer/
â”œâ”€â”€ write_transformer.py      â† wszystko w jednym pliku (EN)
â”œâ”€â”€ write_transformer_PL.py   â† wersja polska
â”œâ”€â”€ README.md
â”œâ”€â”€ README_PL.md              â† wersja polska
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

Jeden plik. Celowo. OtwÃ³rz go w dowolnym edytorze i zobacz wszystko naraz. 
Brak skakania miÄ™dzy moduÅ‚ami. Brak ukrytej zÅ‚oÅ¼onoÅ›ci.

Podczas trenowania powstaje katalog `checkpoint/`:

```text
checkpoint/
â”œâ”€â”€ model.pt        â† wytrenowane wagi
â”œâ”€â”€ tokenizer.json  â† sÅ‚ownik BPE i reguÅ‚y Å‚Ä…czenia
â””â”€â”€ config.json     â† konfiguracja architektury
```

---

## â“ FAQ

**Czy to jest "prawdziwy" Transformer?**
Tak. To dokÅ‚adnie ta sama architektura co GPT-2/3/4: multi-head attention, pre-norm, GELU, weight tying, tokenizer BPE. RÃ³Å¼ni siÄ™ tylko skalÄ….

**Czy potrzebujÄ™ karty graficznej (GPU)?**
Nie. Kurs trenuje siÄ™ na procesorze (CPU) w kilka sekund. Przy wiÄ™kszych tekstach GPU pomaga, ale nie jest wymagane.

**Model generuje bzdury!**
To normalne przy maleÅ„kich modelach i maÅ‚ej iloÅ›ci danych. SprÃ³buj obniÅ¼yÄ‡ temperaturÄ™ (`/temp 0.3`), zwiÄ™kszyÄ‡ liczbÄ™ epok lub dodaÄ‡ wiÄ™cej tekstu. Celem jest zrozumienie architektury, a nie walka z ChatGPT.

**Czy mogÄ™ trenowaÄ‡ na angielskim tekÅ›cie?**
Tak. DomyÅ›lny korpus jest polski, ale tokenizer BPE dziaÅ‚a w kaÅ¼dym jÄ™zyku. UÅ¼yj flagi `--train`.

---

<div align="center">
JeÅ›li ten projekt pomÃ³gÅ‚ Ci zrozumieÄ‡ Transformery â€” zostaw â­!<br><br>
<i>Nie musisz rozumieÄ‡ 1.8 biliona parametrÃ³w.<br>
Zrozum 50 tysiÄ™cy â€” reszta to ta sama architektura, tylko wiÄ™ksza.</i>
</div>
